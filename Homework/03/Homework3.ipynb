{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADA 2018 - Homework 3\n",
    "\n",
    "\n",
    "\n",
    "## Undestanding the StackOverflow community\n",
    "\n",
    "\n",
    "Deadline: Nov 7th 2018, 23:59:59\n",
    "\n",
    "Submission link: Check channel homework-3-public"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StackOverflow is the most popular programming-related Q&A website. It serves as a platform for users to ask and answer questions and to vote questions and answers up or down. Users of StackOverflow can earn reputation points and \"badges\"; for example, a person is awarded 10 reputation points for receiving an \"up\" vote on an answer given to a question, and 5 points for the \"up\" vote on a question asked. Also, users receive badges for their valued contributions, which represents a kind of gamification of the traditional Q&A site. \n",
    "\n",
    "[Learn more about StackOverflow on Wikipedia](https://en.wikipedia.org/wiki/Stack_Overflow)\n",
    "\n",
    "----\n",
    "\n",
    "Dataset link:\n",
    "\n",
    "https://drive.google.com/open?id=1POlGjqzw9v_pZ_bUnXGihOgk45kbvNjB\n",
    "\n",
    "http://iccluster053.iccluster.epfl.ch/Posts.json.zip (mirror 1)\n",
    "\n",
    "https://iloveadatas.com/datasets/Posts.json.zip (mirror 2)\n",
    "\n",
    "Dataset description:\n",
    "\n",
    "* **Id**: Id of the post\n",
    "* **CreationDate**: Creation date of the post (String format)\n",
    "* **PostTypeId**: Type of post (Question = 1, Answer = 2)\n",
    "* **ParentId**: The id of the question. Only present if PostTypeId = 2\n",
    "* **Score**: Points assigned by the users\n",
    "* **Tags**: Tags of the question. Only present if PostTypeId = 1\n",
    "* **Title**: Only present if PostTypeId = 1\n",
    "* **ViewCount**: Only present if PostTypeId = 1\n",
    "\n",
    "The dataset format is JSON. Here are examples of a question and an answer:\n",
    "\n",
    "Question:\n",
    "```json\n",
    "{\n",
    "    \"Id\": 10130734,\n",
    "    \"CreationDate\": \"2012-04-12T19:51:25.793+02:00\",\n",
    "    \"PostTypeId\": 1,\n",
    "    \"Score\": 4,\n",
    "    \"Tags\": \"<python><pandas>\",\n",
    "    \"Title\": \"Best way to insert a new value\",\n",
    "    \"ViewCount\": 3803\n",
    "}\n",
    "```\n",
    "\n",
    "Answer:\n",
    "```json\n",
    "{  \n",
    "   \"CreationDate\":\"2010-10-26T03:19:05.063+02:00\",\n",
    "   \"Id\":4020440,\n",
    "   \"ParentId\":4020214,\n",
    "   \"PostTypeId\":2,\n",
    "   \"Score\":1\n",
    "}\n",
    "```\n",
    "\n",
    "----\n",
    "Useful resources:\n",
    "\n",
    "**Spark SQL, DataFrames and Datasets Guide**\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "\n",
    "**Database schema documentation for the public data dump**\n",
    "\n",
    "https://meta.stackexchange.com/questions/2677/database-schema-documentation-for-the-public-data-dump-and-sede\n",
    "\n",
    "----\n",
    "\n",
    "**Note:** Use Spark where possible. Some computations can take more than 10 minutes on a common notebook. Consider to save partial results on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your imports here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import col\n",
    "%matplotlib inline\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task A: Convert the dataset to a more convenient format\n",
    "As a warm-up task (and to avoid to warm up your laptop too much), load the dataset into a Spark dataframe, show the content, and save it in the _Parquet_ format. Use this step to convert the fields to a more convenient form.\n",
    "\n",
    "Answer the following questions:\n",
    "\n",
    "1. How many questions have been asked on StackOverflow?\n",
    "2. How many answers have been given?\n",
    "3. What is the percentage of questions with a score of 0?\n",
    "\n",
    "**Hint:** The next tasks involve a time difference. Consider storing time in numeric format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_corrupt_record: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add your code and description here\n",
    "posts = spark.read.json(\"Posts.json.zip\")\n",
    "posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the DataFrame:\n",
      "_corrupt_record\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns in the DataFrame:\")\n",
    "for column in posts.columns:\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `PostTypeId` cannot be resolved. Did you mean one of the following? [`_corrupt_record`].;\n'Filter ('PostTypeId = 1)\n+- Relation [_corrupt_record#34] json\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 1. How many questions have been asked on StackOverflow?\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m questions_count \u001b[38;5;241m=\u001b[39m \u001b[43mposts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPostTypeId\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcount()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 2. How many answers have been given?\u001b[39;00m\n\u001b[0;32m      5\u001b[0m answers_count \u001b[38;5;241m=\u001b[39m posts\u001b[38;5;241m.\u001b[39mfilter(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPostTypeId\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcount()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\ada\\lib\\site-packages\\pyspark\\sql\\dataframe.py:3325\u001b[0m, in \u001b[0;36mDataFrame.filter\u001b[1;34m(self, condition)\u001b[0m\n\u001b[0;32m   3323\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mfilter(condition)\n\u001b[0;32m   3324\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(condition, Column):\n\u001b[1;32m-> 3325\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcondition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3326\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m   3328\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN_OR_STR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3329\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcondition\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(condition)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m   3330\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\ada\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\ada\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `PostTypeId` cannot be resolved. Did you mean one of the following? [`_corrupt_record`].;\n'Filter ('PostTypeId = 1)\n+- Relation [_corrupt_record#34] json\n"
     ]
    }
   ],
   "source": [
    "# 1. How many questions have been asked on StackOverflow?\n",
    "questions_count = posts.filter(col(\"PostTypeId\") == 1).count()\n",
    "\n",
    "# 2. How many answers have been given?\n",
    "answers_count = posts.filter(col(\"PostTypeId\") == 2).count()\n",
    "\n",
    "# 3. What is the percentage of questions with a score of 0?\n",
    "total_questions = posts.filter(col(\"PostTypeId\") == 1).count()\n",
    "zero_score_questions = posts.filter((col(\"PostTypeId\") == 1) & (col(\"Score\") == 0)).count()\n",
    "\n",
    "percentage_zero_score_questions = (zero_score_questions / total_questions) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint:** Load the dataset from the Parquet file for the next tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task B: What are the 10 most popular tags?\n",
    "\n",
    "What are the most popular tags in StackOverflow? Use Spark to extract the information you need, and answer the following questions with Pandas and Matplotlib (or Seaborn):\n",
    "\n",
    "1. What is the proportion of tags that appear in fewer than 100 questions?\n",
    "2. Plot the distribution of the tag counts using an appropriate representation.\n",
    "3. Plot a bar chart with the number of questions for the 10 most popular tags.\n",
    "\n",
    "For each task describe your findings briefly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split\n",
    "\n",
    "# Explode the Tags column to get individual tags\n",
    "tags_df = posts.select(\"Tags\").withColumn(\"Tag\", explode(split(\"Tags\", \"><\")))\n",
    "\n",
    "# Remove angle brackets from tags\n",
    "tags_df = tags_df.withColumn(\"Tag\", tags_df[\"Tag\"].substr(2, len(tags_df[\"Tag\"]) - 2))\n",
    "\n",
    "# Group by tags and count the occurrences\n",
    "tag_counts = tags_df.groupBy(\"Tag\").count()\n",
    "\n",
    "# Cache the DataFrame for better performance\n",
    "tag_counts.cache()\n",
    "\n",
    "# Show the top 10 most popular tags\n",
    "top_tags = tag_counts.orderBy(\"count\", ascending=False).limit(10)\n",
    "top_tags.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code and description here\n",
    "top_tags_pd = top_tags.toPandas()\n",
    "\n",
    "# 1. What is the proportion of tags that appear in fewer than 100 questions?\n",
    "less_than_100 = tag_counts.filter(\"count < 100\").count()\n",
    "total_tags = tag_counts.count()\n",
    "proportion_less_than_100 = less_than_100 / total_tags\n",
    "\n",
    "print(\"Proportion of tags appearing in fewer than 100 questions:\", proportion_less_than_100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 2. Plot the distribution of tag counts using a histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(tag_counts.select(\"count\").rdd.flatMap(lambda x: x).collect(), bins=50, color=\"skyblue\", edgecolor=\"black\")\n",
    "plt.title(\"Distribution of Tag Counts\")\n",
    "plt.xlabel(\"Number of Questions\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Plot a bar chart with the number of questions for the 10 most popular tags\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(top_tags_pd[\"Tag\"], top_tags_pd[\"count\"], color=\"orange\")\n",
    "plt.title(\"Top 10 Most Popular Tags\")\n",
    "plt.xlabel(\"Tag\")\n",
    "plt.ylabel(\"Number of Questions\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task C: View-score relation\n",
    "\n",
    "We want to investigate the correlation between the view count and the score of questions.\n",
    "\n",
    "1. Get the view count and score of the questions with tag ```random-effects``` and visualize the relation between these two variables using an appropriate plot.\n",
    "2. Are these two variables correlated? Use the Pearson coefficient to validate your hypothesis. Discuss your findings in detail.\n",
    "\n",
    "**Hint:** Inspect the data visually before drawing your conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code and description here\n",
    "\n",
    "import seaborn as sns \n",
    "\n",
    "# Filter questions with the tag 'random-effects'\n",
    "random_effects_questions = posts.filter((col(\"PostTypeId\") == 1) & (col(\"Tags\").contains(\"<random-effects>\")))\n",
    "\n",
    "# Select only the relevant columns (ViewCount and Score)\n",
    "view_score_data = random_effects_questions.select(\"ViewCount\", \"Score\").na.drop()\n",
    "\n",
    "# Convert to Pandas for visualization\n",
    "view_score_pd = view_score_data.toPandas()\n",
    "\n",
    "# 1. Visualize the relation between ViewCount and Score using a scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=\"ViewCount\", y=\"Score\", data=view_score_pd)\n",
    "plt.title(\"View Count vs. Score for Questions with Tag 'random-effects'\")\n",
    "plt.xlabel(\"View Count\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.show()\n",
    "\n",
    "# 2. Calculate the Pearson correlation coefficient\n",
    "correlation_coefficient = view_score_pd[\"ViewCount\"].corr(view_score_pd[\"Score\"], method=\"pearson\")\n",
    "print(\"Pearson Correlation Coefficient:\", correlation_coefficient)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task D: What are the tags with the fastest first answer?\n",
    "\n",
    "What are the tags that have the fastest response time from the community? We define the response time as the difference in seconds between the timestamps of the question and of the first answer received.\n",
    "\n",
    "1. Get the response time for the first answer of the questions with the tags ```python``` and ```java```.\n",
    "2. Plot the two distributions in an appropriate format. What do you observe? Describe your findings and discuss the following distribution properties: mean, median, standard deviation.\n",
    "3. We believe that the response time is lower for questions related to Python (compare to Java). Contradict or confirm this assumption by estimating the proper statistic with bootstrapping. Visualize the 95% confidence intervals with box plots and describe your findings.\n",
    "3. Repeat the first analysis (D1) by using the proper statistic to measure the response time for the tags that appear at least 5000 times. Plot the distribution of the 10 tags with the fastest response time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code and description here\n",
    "from scipy.stats import ttest_ind\n",
    "from pyspark.sql.functions import col, when, min, unix_timestamp\n",
    "\n",
    "# Filter questions with the tags 'python' and 'java'\n",
    "selected_tags = ['python', 'java']\n",
    "selected_questions = posts.filter((col(\"PostTypeId\") == 1) & col(\"Tags\").contains(\"|\".join(selected_tags)))\n",
    "\n",
    "# Calculate the response time for the first answer\n",
    "response_time_data = selected_questions.select(\n",
    "    \"Id\", \"CreationDate\",\n",
    "    min(when(col(\"PostTypeId\") == 2, col(\"CreationDate\"))).over(Window.partitionBy(\"Id\")).alias(\"FirstAnswerDate\")\n",
    ")\n",
    "\n",
    "response_time_data = response_time_data.withColumn(\n",
    "    \"ResponseTime\",\n",
    "    (unix_timestamp(\"FirstAnswerDate\") - unix_timestamp(\"CreationDate\")).cast(\"int\")\n",
    ").na.drop()\n",
    "\n",
    "# Convert to Pandas for visualization\n",
    "response_time_pd = response_time_data.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Plot the distributions for Python and Java\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.histplot(response_time_pd, x=\"ResponseTime\", hue=\"Tags\", element=\"step\", common_norm=False, stat=\"density\", kde=True)\n",
    "plt.title(\"Distribution of Response Time for Python and Java Questions\")\n",
    "plt.xlabel(\"Response Time (seconds)\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate mean, median, and standard deviation\n",
    "mean_python = response_time_pd[response_time_pd['Tags'] == 'python']['ResponseTime'].mean()\n",
    "median_python = response_time_pd[response_time_pd['Tags'] == 'python']['ResponseTime'].median()\n",
    "std_python = response_time_pd[response_time_pd['Tags'] == 'python']['ResponseTime'].std()\n",
    "\n",
    "mean_java = response_time_pd[response_time_pd['Tags'] == 'java']['ResponseTime'].mean()\n",
    "median_java = response_time_pd[response_time_pd['Tags'] == 'java']['ResponseTime'].median()\n",
    "std_java = response_time_pd[response_time_pd['Tags'] == 'java']['ResponseTime'].std()\n",
    "\n",
    "print(\"Statistics for Python:\")\n",
    "print(\"Mean:\", mean_python)\n",
    "print(\"Median:\", median_python)\n",
    "print(\"Standard Deviation:\", std_python)\n",
    "\n",
    "print(\"\\nStatistics for Java:\")\n",
    "print(\"Mean:\", mean_java)\n",
    "print(\"Median:\", median_java)\n",
    "print(\"Standard Deviation:\", std_java)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Use bootstrapping to estimate the 95% confidence intervals and visualize with box plots\n",
    "def bootstrap_ci(data, n_bootstrap=1000, alpha=0.05):\n",
    "    bootstrapped_means = []\n",
    "    for _ in range(n_bootstrap):\n",
    "        bootstrap_sample = np.random.choice(data, size=len(data), replace=True)\n",
    "        bootstrapped_means.append(np.mean(bootstrap_sample))\n",
    "\n",
    "    lower_ci = np.percentile(bootstrapped_means, alpha / 2 * 100)\n",
    "    upper_ci = np.percentile(bootstrapped_means, (1 - alpha / 2) * 100)\n",
    "\n",
    "    return lower_ci, upper_ci\n",
    "\n",
    "# Bootstrap and calculate confidence intervals for Python and Java\n",
    "python_ci = bootstrap_ci(response_time_pd[response_time_pd['Tags'] == 'python']['ResponseTime'])\n",
    "java_ci = bootstrap_ci(response_time_pd[response_time_pd['Tags'] == 'java']['ResponseTime'])\n",
    "\n",
    "# Visualize the 95% confidence intervals with box plots\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=\"Tags\", y=\"ResponseTime\", data=response_time_pd)\n",
    "plt.title(\"Response Time Distribution with 95% Confidence Intervals\")\n",
    "plt.xlabel(\"Tags\")\n",
    "plt.ylabel(\"Response Time (seconds)\")\n",
    "plt.ylim(0, 1000)  # Adjust ylim for better visualization\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n95% Confidence Intervals:\")\n",
    "print(\"Python:\", python_ci)\n",
    "print(\"Java:\", java_ci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Repeat the first analysis (D1) for the tags that appear at least 5000 times\n",
    "tag_counts = posts.filter(col(\"PostTypeId\") == 1).groupBy(\"Tags\").count().filter(\"count >= 5000\")\n",
    "\n",
    "# Extract the tags with at least 5000 occurrences\n",
    "popular_tags = [row[\"Tags\"] for row in tag_counts.collect()]\n",
    "\n",
    "# Filter questions with popular tags\n",
    "popular_tags_questions = posts.filter((col(\"PostTypeId\") == 1) & col(\"Tags\").isin(popular_tags))\n",
    "\n",
    "# Calculate the response time for the first answer\n",
    "response_time_popular_tags = popular_tags_questions.select(\n",
    "    \"Id\", \"CreationDate\",\n",
    "    min(when(col(\"PostTypeId\") == 2, col(\"CreationDate\"))).over(Window.partitionBy(\"Id\")).alias(\"FirstAnswerDate\")\n",
    ")\n",
    "\n",
    "response_time_popular_tags = response_time_popular_tags.withColumn(\n",
    "    \"ResponseTime\",\n",
    "    (unix_timestamp(\"FirstAnswerDate\") - unix_timestamp(\"CreationDate\")).cast(\"int\")\n",
    ").na.drop()\n",
    "\n",
    "# Convert to Pandas for visualization\n",
    "response_time_popular_tags_pd = response_time_popular_tags.toPandas()\n",
    "\n",
    "# Select the top 10 tags with the fastest response time\n",
    "top_10_tags = response_time_popular_tags_pd.groupby(\"Tags\").mean().sort_values(by=\"ResponseTime\").head(10)\n",
    "\n",
    "# Plot the distribution of the 10 tags with the fastest response time\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=top_10_tags.index, y=top_10_tags[\"ResponseTime\"], palette=\"viridis\")\n",
    "plt.title(\"Top 10 Tags with the Fastest Response Time\")\n",
    "plt.xlabel(\"Tag\")\n",
    "plt.ylabel(\"Average Response Time (seconds)\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task E: What's up with PySpark?\n",
    "The number of questions asked regarding a specific topic reflect the public’s interest on it. We are interested on the popularity of PySpark. Compute and plot the number of questions with the ```pyspark``` tag for 30-day time intervals. Do you notice any trend over time? Is there any correlation between time and number of questions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code and description here\n",
    "\n",
    "# Filter questions with the 'pyspark' tag\n",
    "pyspark_questions = posts.filter((col(\"PostTypeId\") == 1) & col(\"Tags\").contains(\"<pyspark>\"))\n",
    "\n",
    "# Extract the creation date and convert it to date format\n",
    "pyspark_questions = pyspark_questions.withColumn(\"CreationDate\", to_date(col(\"CreationDate\")))\n",
    "\n",
    "# Group by 30-day intervals and count the number of questions\n",
    "questions_per_interval = pyspark_questions.groupBy(window(\"CreationDate\", \"30 days\")).agg(count(\"*\").alias(\"NumQuestions\"))\n",
    "\n",
    "# Convert to Pandas for visualization\n",
    "questions_per_interval_pd = questions_per_interval.toPandas()\n",
    "\n",
    "# Plot the number of questions with the 'pyspark' tag over time\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(questions_per_interval_pd[\"window.start\"], questions_per_interval_pd[\"NumQuestions\"], marker='o', linestyle='-')\n",
    "plt.title(\"Number of Questions with 'pyspark' Tag Over Time (30-day Intervals)\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Number of Questions\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# stop the spark session\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39mstop()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# stop the spark session\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
